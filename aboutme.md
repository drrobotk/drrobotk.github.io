---
layout: post
title: About me
subtitle: whoami
date: 2022-03-15
last-updated: 2022-09-24
---
<p align="center"><img src="/assets/img/neurodiversity.png" alt="isolated" width="200"/></p>


I am a Data Scientist with an MSci in Mathematics, with First Class Honours, and a PhD in Applied Mathematics and Theoretical Physics, and as such I have a lot of analytical experience including theoretical and applied techniques in mathematics, statistical and numerical methods. 

<p align="center"><img src="/assets/img/tp.jpeg" alt="isolated" width="350"/></p>

I am an expert in Python code development, creating analytical data pipelines (ETL, RAP) and deploying machine learning models at scale, using cloud services (e.g GCP, Cloudera) and technologies such as Apache Spark or BigQuery with SQL.

<p align="center"><img src="/assets/img/pysym.jpg" alt="isolated" width="200"/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="/assets/img/coding.gif" alt="isolated" width="400"/></p>
<p align="center"><img src="https://www.databricks.com/wp-content/uploads/2021/05/ETL-Process.jpg" alt="isolated" width="350"/></p>

{% include mathjax.html type="post" %}

For my MSci dissertation, I studied integrable quantum spin chains. This introduced me to quantum interaction models particularly the Heisenberg spin chain, which describes the nearest neighbour interaction of particles with spin-$\frac{1}{2}$  (i.e electrons) and naturally arises in the study of ferromagnetism. 

<p align="center">
<img src="/assets/img/magnon.gif" alt="isolated" width="550"/>
</p>


These models have acquired growing importance in [quantum computing](https://doi.org/10.48550/arXiv.2207.09994) within the field of quantum information processing, mainly as a means of efficiently transferring information. It also gave me insights into various mathematical methods to diagonalize large square matrices which can grow exponentially according to the number of particles. 

<p align="center">
<img src="https://www.researchgate.net/publication/328536318/figure/fig1/AS:685999256506368@1540566376509/Diagram-of-ideal-quantum-computer-Information-about-its-solution-is-in-the-final-vector.png" alt="isolated" width="300"/>&nbsp;&nbsp;&nbsp;&nbsp;
<img src="https://miro.medium.com/max/800/1*KZj_DViOL0gEyK8B_TkRUg.gif" alt="isolated" width="300"/>
</p>

For my academic excellence in my MSci, I was also awarded a scholarship by the [Science and Technology Facilities Council](https://stfc.ukri.org/) (STFC) to undertake a PhD in Applied Mathematics and Theoretical Physics under the mathematics department. The main theme of my [PhD research](https://doi.org/10.48550/arXiv.1910.01080) was a study of the symmetries of black hole horizons in quantum gravity.

<p align="center">
<img src="/assets/img/qg.jpg" alt="isolated" width="375"/>
<img src="/assets/img/source.gif" alt="isolated" width="300"/>
</p>

In particular, under various string and supergravity theories, specifically [type IIA](https://doi.org/10.1007/JHEP06(2015)139), [massive type IIA](https://doi.org/10.1088/0264-9381/32/23/235004) and [5-dimensional](https://doi.org/10.1088/1361-6382/aac30c) supergravities (both gauged and ungauged). The work has been published in three peer-review papers in leading international journals with the third publication on the five-dimensional supergravity theories being a sole-author paper. My examiners remarked at the great achievement to produce such a sole-author paper during a PhD. I had also continued my research and publications in quantum gravity as an independent researcher for black holes in [6-dimensional](https://doi.org/10.48550/arXiv.1912.04249) gauged $N=(1,0)$ supergravity.

<p align="center"><img src="/assets/img/stringt.png" alt="isolated" width="350"/></p>

I have experience coding in many languages (e.g Python, C++) using various OS and I have extensively used computational software (e.g Matlab, Mathematica, Maple) or libraries (e.g numpy, scipy, matplotlib) for performing mathematical, physical, and statistical computations on various analyses and datasets. 

<p align="center"><img src="/assets/img/wolfram.png" alt="isolated" width="250"/></p>
<p align="center"><img src="https://miro.medium.com/max/1400/1*OTQHk3rsuzwdidO9zgSOfA.png" alt="isolated" width="350"/></p>

For my PhD research, I had performed extensive computations on multi-dimensional arrays for supergravity calculations using Python with [Cadabra](https://cadabra.science/). 

<p align="center"><img src="/assets/img/cadabra.jpg" alt="isolated" width="350"/></p>


I also have experience teaching and tutoring Mathematics and Physics at a Graduate (BSc/MSc), A-level and GCSE. I am currently tutoring university students in mathematics topics such as advanced statistics, linear algebra and Python coding.

I am also experienced with data science and machine learning R and Python libraries (e.g Scikit-learn, Keras, Tensorflow) as well as data visualization tools such as Tableau. As a freelance data scientist, I analysed instantaneous power consumption data of a large number of households with ML to identify various devices (e.g the TV or kettle) and classify when they are turned on and the occurrences/duration of their usage, to identify routines and detection of anomalies. This data was provided by a particular company that aims to use ML and modelling with domestic electric appliances.

<p align="center"><img src="https://camo.githubusercontent.com/aeb4f612bd9b40d81c62fcbebd6db44a5d4344b8b962be0138817e18c9c06963/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74665f6c6f676f5f686f72697a6f6e74616c2e706e67" alt="isolated" width="300"/>
<img src="https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/scikit-learn-logo.png" alt="isolated" width="150"/></p>

In **June 2021**, I was employed as a Data Scientist at the *Higher Executive Officer* grade at the Office for National Statistics (ONS) within the Economics Statistics Group (ESG) and Emerging Platforms Delivery Support (EPDS). 

<p align="center"><img src="/assets/img/ons.png" alt="isolated" width="275"/></p>

After only my second month at the ONS, I was a member of the induction team responsible for onboarding new starters and aiding or mentoring to new members of the team. My main work was researching and implementing multilateral price indices, using calculations and time series extension methods in Python. This work was as part of an ETL Reproducible Analytical Pipeline (RAP) on Cloudera with Apache Spark for the treatment of [alternative data sources](https://www.ons.gov.uk/economy/inflationandpriceindices/articles/introducingalternativedataintoconsumerpricestatisticsaggregationandweights/2021-11-09) (scanner and web-scraped data) and [new index methods](https://www.ons.gov.uk/economy/inflationandpriceindices/articles/newindexnumbermethodsinconsumerpricestatistics/2020-09-01) (e.g multilateral methods) which will be used to determine the consumer price index (CPI) in the future. 

<p align="center"><img src="/assets/img/spark.png" alt="isolated" width="400"/></p>


While working on building a data pipeline for the CPI, I made very significant contributions both to methodology and computational efficiency for the integration of alternative data sources. In my first few months, I led an investigation into a particular implicit hedonic multilateral index method known as the [Time Product Dummy](https://onlinelibrary.wiley.com/doi/full/10.1111/roiw.12468) (TPD) method, which uses a log-linear price model with weighted least squares regression and expenditure shares as weights:

\begin{equation}
\ln p_i^{t} = \alpha + \sum_{r=1}^T \delta^r D_i^r + \sum_{j=1}^{N-1}\gamma_j D_j + \epsilon_i^{t} \ . 
\end{equation}

After noticing an error in the formulae and example workbooks produced for these methods and bringing this to the attention of the ONS, I worked closely with people from methodology on making sure we got all the technical details right.

My first task was to implement the TPD method within the CPI pipeline using PySpark. This also led me toward discovering [Pandas UDFs](https://www.databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html), which allow for vectorized operations on Big Data and increase performance by up to 100x compared to regular UDFs, and have since been implemented in various multilateral index methods and an integral part of the CPI pipeline. 

<p align="center"><img src="https://kontext.tech/api/flex/medias/obj-1563" alt="isolated" width="500"/></p>

<p align="center"><img src="https://www.databricks.com/wp-content/uploads/2017/10/image1-4.png" alt="isolated" width="500"/></p>

I also used the same ideas for the [Time Dummy Hedonic]((https://onlinelibrary.wiley.com/doi/full/10.1111/roiw.12468)) (TDH) method, which is an explicit hedonic model similar to TPD, but also uses the item characteristics in the WLS regression model. 

\begin{equation}
\ln p_i^{t} = \alpha + \sum_{r=1}^T \delta^r D_i^r + \sum_{k=1}^K \beta_k z_{ik} + \epsilon_i^{t} \ . 
\end{equation}

After implementing the TPD and TDH methods, I turned my attention to another multilateral method known as [Geary-Khamis](https://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/a-comparison-of-ten-methods-for-multilateral-international-price-and-volume-comparison.pdf) (GK) and the usual method involves iteratively calculating the set of quality adjustment factors simultaneously with the price levels. 

$$
\begin{aligned}
b_{n}&=\sum_{t=1}^{T}\left[\frac{q_{t n}}{q_{n}}\right]\left[\frac{p_{t n}}{P_{t}}\right] \ ,
\nonumber \\
P_{t}&=\frac{p^{t} \cdot q^{t}}{ \vec{b} \cdot q^{t}} \ .
\end{aligned}
$$


I was able to independently research and implement a method solely based on [matrix operations](https://drrobotk.github.io/2021-09-20-Geary-Khamis/), which makes the method more efficient since it has vectorized operations which act on the entire data. I also refactored my code for TPD and TDH using matrix operations, which turned out to be more efficient and increased performance by up to 7x compared to standard statistical libraries.

In **September 2021**, after working closely with methodology on index numbers, I was invited to join the [Index Numbers Expert Group](https://gss.civilservice.gov.uk/guidance/methodology/information-on-specific-methods/#index-numbers) (INEG) and the [Data Science and High-performance computing](https://gss.civilservice.gov.uk/guidance/methodology/information-on-specific-methods/#data-science-and-high-performance-computing) (DaSH) expert group. I also delivered a presentation in a seminar to my team and deparment, to introduce the concept of Pandas UDFs. This turned out to be a success as I got good engagement and questions after the presentation, as well as interest from other parties in DaSH, to watch the recording and slides. I also presented a seminar aimed at people both little and extensive knowledge of the subjects, and a Jupyter Notebook of worked examples. I discussed this material with a computing specialist, and with their feedback have produced useful material with a full set of instructions and worked examples, which is accessible to a wider audience.

In **March 2022**, I joined the [Data Science Campus](https://datasciencecampus.ons.gov.uk/) at the ONS with a promotion to *Senior Executive Officer*.

<p align="center"><img src="/assets/img/dsc.png" alt="isolated" width="200"/></p>

My first project was on the [least cost index](https://www.ons.gov.uk/economy/inflationandpriceindices/articles/trackingthelowestcostgroceryitemsukexperimentalanalysis/april2021toapril2022), which was published in May 2022. 

<p align="center"><img src="/assets/img/lci.png" alt="isolated" width="600"/></p>

I played a significant role in researching and implementing the price index and aggregation methods, which was powered by an existing price index package which I created called [PriceIndexCalc](https://pypi.org/project/PriceIndexCalc/). This was used to track the prices over time of the lowest-cost grocery items for 30 products over multiple retailers, using web-scraped data and a data pipeline on the [Google Cloud Platform](https://cloud.google.com/). This analysis was conducted as part of the ONS's current and future analytical work related to the cost of living.

<p align="center"><img src="/assets/img/gcp.jpg" alt="isolated" width="175"/></p>
<p align="center"><img src="/assets/img/bucket.png" alt="isolated" width="600"/></p>

In **April 2022**, I also joined the Data Access Platform Capability And Training Support (DAPCATS) as a mentor, where I have been helping other data scientists and analysts with their work and projects. 

<p align="center"><img src="https://best-practice-and-impact.github.io/ons-spark/_static/logo.png" alt="isolated" width="200"/></p>

I also took part in the *Spark at the ONS* event hosted by DAPCATS and created for the launch of a new [online book](https://best-practice-and-impact.github.io/ons-spark/intro.html). This event was used to discuss various topics and resources related to Spark and Big Data, and I delivered a presentation titled *Spark application debugging, tuning and optimization*. For this talk, I discussed various tips and techniques to increase efficiency, identify bugs or bottlenecks that can cause Spark applications to be slow or fail, and tuning Spark parameters accordingly. This can help to reduce overall developer and compute time, costs for resources to run the Spark application or the environmental impact that comes with using unnecessary extra resources or having significantly longer runtimes. 

In **August 2022**, I received the Recognition Award for outstanding collaboration and contribution to the ONS. I provided very important support to help another team to publish the [Capital Stocks user guide](https://www.ons.gov.uk/releases/introducingthecapitalstocksuserguide) article and the work has made the UK the only country to introduce such transparency. The process involved sharing their [statistical production code](https://github.com/ONSdigital/Capstocks) in the ONS's GitHub account and I dedicated my time to help them set up the initial account, and to upload the packages in GitHub as the team hadn't experienced using this platform before. I also took the time to give them a very detailed walk through of how the platform works, and helped them by sharing tips and examples of good practice. My support enabled them to make their capital stocks statistical production system accessible and reproducible by all external users, helping them make the statistics more inclusive and introducing innovating platforms to help their users improve their analysis and budgetary forecasting.
