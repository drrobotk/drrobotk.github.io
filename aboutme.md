---
layout: post
title: About me
subtitle: whoami
date: 2022-04-15
last-updated: 2023-05-29
---
# Table of Contents
1. [Introduction](#introduction)
2. [Academia and Research](#academia)
3. [Tutoring and Content Development](#tutoring)
4. [Coding and Data Science](#coding_data_science)
5. [My journey at the Office for National Statistics](#ons_journey)
6. [My journey at Quantexa](#quantexa_journey)

<a name="introduction"></a>
# Introduction

Hello, I'm **Usman Kayani** — a seasoned Senior Data Engineer and Scientist with a rich academic foundation in Mathematics, reflected in my MSci and PhD degrees. Throughout my career, I've garnered invaluable analytical experience, mastering both theoretical and applied techniques across diverse fields such as mathematics, statistics, and numerical methods.

<p align="center"><img src="/assets/img/tp2.webp" alt="isolated" width="275"/></p>

Armed with over a decade of Python development expertise and a recent proficiency in Scala, my specialty lies in engineering robust analytical data pipelines (ETL, RAP) and skillfully deploying machine learning models at scale. My proficiency in harnessing cloud services like GCP, Cloudera, and AWS, coupled with my adept usage of technologies such as Apache Spark, BigQuery, and SQL, enables me to design dynamic, scalable data solutions.

<p align="center"><img src="/assets/img/pysym.jpg" alt="isolated" width="200"/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="/assets/img/coding.gif" alt="isolated" width="400"/></p>
<p align="center"><img src="https://www.databricks.com/wp-content/uploads/2021/05/ETL-Process.jpg" alt="isolated" width="350"/></p>

I identify as [neurodivergent](https://exceptionalindividuals.com/neurodiversity/), living with [ADHD](https://www.nhs.uk/conditions/attention-deficit-hyperactivity-disorder-adhd/) and [Dyslexia](https://www.bdadyslexia.org.uk/dyslexia/about-dyslexia/what-is-dyslexia), and find that this unique perspective has richly informed my life and career. As an advocate for diversity and inclusion, I am passionate about fostering an environment of acceptance and understanding for neurodiversity in the [workplace](https://www.inclusiveemployers.co.uk/blog/neurodiversity-in-the-workplace-understanding-is-key/).


<p align="center"><img src="/assets/img/neurodiversity.png" alt="isolated" width="200"/></p>

Fuelled by my unwavering belief in the transformative potential of data and technology, I continuously strive to apply my skills towards positive societal change. I am always on the lookout for innovative opportunities where data science can be leveraged to improve lives and create a meaningful impact.

{% include mathjax.html type="post" %}

<a name="academia"></a>
# Academia and Research

I completed an MSci in Mathematics, with **First Class Honours**, and a PhD in Applied Mathematics and Theoretical Physics at [King's College London](https://www.kcl.ac.uk/). 

<p align="center"><a href="https://www.kcl.ac.uk/"><img src="/assets/img/kcl.jpeg" alt="isolated" width="150"/></a></p>

During my MSci, I scored highly in my modules, most notably gaining 99% in *linear algebra* and *partial differential equations with complex variables*. Other pure and applied mathematics [modules](https://web.archive.org/web/20090221212207/http://www.mth.kcl.ac.uk/courses) that I completed included topics covering advanced calculus, dynamical systems, probability and statistics, discrete mathematics, logic, complex analysis, manifolds, linear algebra, differential geometry and mathematical biology.

<p align="center"><img src="/assets/img/math.jpeg" alt="isolated" width="600"/></p>


In my second year as an undergraduate, I had written a paper under the advice of one of my professors, who thought that it was interesting enough to be posted on the [university website](https://web.archive.org/web/20100113041557/http://www.mth.kcl.ac.uk:80/courses/cm211.html) for the mathematics department. This gave me valuable experience on how to write a paper and investigate different mathematical techniques and ideas.

The paper I wrote was on a new method to solve linear ordinary differential equations of any order with non-constant coefficients and certain classes of partial differential equations. As a result, by the recommendation of another professor and based on my academic record, I was able to attend the fourth year module, [Fourier analysis](https://web.archive.org/web/20090307084310/http://www.mth.kcl.ac.uk/courses/cmms11.html), for extra credit in my second year.

<p align="center"><img src="/assets/img/ft.gif" alt="isolated" width="425"/></p>

In the final two years of my MSci, I also completed modules in physics, covering quantum mechanics, advanced quantum field theory, statistical mechanics, advanced general relativity, string theory, and cosmology.

<p align="center"><img src="/assets/img/eqn.jpeg" alt="isolated" width="600"/></p>

For my MSci dissertation, I studied integrable quantum spin chains. This introduced me to quantum interaction models particularly the Heisenberg spin chain, which describes the nearest neighbour interaction of particles with spin-$\frac{1}{2}$  (i.e electrons) and naturally arises in the study of ferromagnetism. 

<p align="center">
<img src="/assets/img/magnon.gif" alt="isolated" width="550"/>
</p>

These models have acquired growing importance in [quantum computing](https://doi.org/10.48550/arXiv.2207.09994) within the field of quantum information processing, mainly as a means of efficiently transferring information. In addition to the understanding of quantum computing that I acquired from the research, it also gave me insights into various mathematical methods to diagonalize large square matrices which can grow exponentially according to the number of particles. 

<p align="center">
<img src="/assets/img/qc.png" alt="isolated" width="300"/>&nbsp;&nbsp;&nbsp;&nbsp;
<img src="/assets/img/qubit.gif" alt="isolated" width="325"/>
</p>

For my academic excellence in my MSci, I was also awarded a scholarship by the [Science and Technology Facilities Council](https://stfc.ukri.org/) (STFC) to undertake a PhD in Applied Mathematics and Theoretical Physics under the mathematics department. My research was in quantum gravity, which is the study of the relationship between quantum mechanics and general relativity.

<p align="center">
<img src="/assets/img/qg.jpeg" alt="isolated" width="450"/>
</p>

In particular, I studied [string](https://en.wikipedia.org/wiki/String_theory) and [supergravity](https://en.wikipedia.org/wiki/Supergravity) theories, specifically [type IIA](https://doi.org/10.1007/JHEP06(2015)139), [massive type IIA](https://doi.org/10.1088/0264-9381/32/23/235004) and [5-dimensional](https://doi.org/10.1088/1361-6382/aac30c) (both gauged and ungauged). 

<p align="center"><img src="/assets/img/stringt.png" alt="isolated" width="425"/></p>

The main theme of my [PhD research](https://doi.org/10.48550/arXiv.1910.01080) was a study of the *symmetries* of black hole horizons in quantum gravity. 

<p align="center"><img src="/assets/img/bhs.gif" alt="isolated" width="400"/><img src="/assets/img/source.gif" alt="isolated" width="402"/></p>


The work has been published in three peer-review papers in leading international journals with the third publication on the five-dimensional supergravity theories being a sole-author paper. My examiners remarked at the great achievement to produce such a sole-author paper during a PhD. I had also continued my research and publications in quantum gravity as an independent researcher for black holes in [6-dimensional](https://doi.org/10.48550/arXiv.1912.04249) gauged $N=(1,0)$ supergravity. 

<p align="center"><img src="/assets/img/sym.gif" alt="isolated" width="400"/></p>

The methodology used to investigate these problems involves techniques in Lie algebras, differential geometry, differential equations on compact manifolds, general relativity, supersymmetry and string theory. Algebraic and differential topology were also essential in the analysis.

<p align="center"><img src="/assets/img/e8.gif" alt="isolated" width="150"/></p>


For my research in quantum gravity, I also performed extensive computations on multi-dimensional arrays for supergravity calculations using Python with [Cadabra](https://cadabra.science/), particularly for Clifford algebras and spinors in higher dimensions.

<p align="center"><a href="https://cadabra.science/"><img src="/assets/img/cadabra.jpg" alt="isolated" width="350"/></a></p>

I also attended various academic seminars and conferences such as the [Winter School on Supergravity, Strings, and Gauge Theory](https://indico.cern.ch/event/439879/) at [CERN](https://home.web.cern.ch/) in Switzerland, and I presented my research at many conferences including the [Young Theorists’ Forum](https://maths.dur.ac.uk/YTF/8/) at Durham University.

<p align="center"><a href="https://indico.cern.ch/event/439879/"><img src="/assets/img/cern.png" alt="isolated" width="100"/></a></p>

<a name="tutoring"></a>
# Tutoring and Content Development

I have experience teaching and tutoring Mathematics, Physics and Programming at a Graduate (BSc/MSc), A-level and GCSE. I was a graduate teaching assistant at [King's College London](https://www.kcl.ac.uk/) for 6 years, and a private tutor for over 12 years. During my PhD, I also volunteered for roles in science communication, such as with the [Institute of Physics](https://www.iop.org/), to explain my research on black holes to school children.

<p align="center"><a href="https://www.iop.org/"><img src="/assets/img/iop.png" alt="isolated" width="150"/></a></p>

In the last few years, I have also undertaken freelance work with [Witherow Brooke](https://www.witherowbrooke.co.uk/), a private tuition and educational consultancy company which was featured in The Telegraph. I am currently tutoring university students in mathematics topics such as advanced statistics, linear algebra and Python coding.

<p align="center"><a href="https://www.witherowbrooke.co.uk/"><img src="https://images.squarespace-cdn.com/content/v1/5f9b483a3274976f2be7ff49/1629543370730-SJGZ5LRA0O2PYEP44JGH/WBLOGO.png" alt="isolated" width="275"/></a><img src="/assets/img/wb.png" alt="isolated" width="275"/></p>



I also worked as a freelance mathematics content writer and video developer for [Nagwa](https://www.nagwa.com/en/), a leading educational technology company in the Middle East and North Africa. I was responsible for developing educational videos and content for the company's online platform for topics in mathematics from GCSE to graduate level, such as algebra, trigonometry, calculus and statistics.

<p align="center"><a href="https://www.nagwa.com/en/"><img src="/assets/img/nagwa.png" alt="isolated" width="150"/></a></p>

<a name="coding_data_science"></a>
# Coding and Data Science


I have extensive experience coding in many high-level programming languages (e.g C++, Java), scripting languages (e.g Python, R, Bash, etc) and symbolic languages (e.g Mathematica, MATLAB, Maple etc) using various operating systems (e.g Linux, Windows, Mac OS). 

<p align="center"><a href="https://www.wolfram.com/mathematica/"><img src="/assets/img/wolfram.gif" alt="isolated" width="575"/></a></p>
<p align="center"><img src="/assets/img/R.gif" alt="isolated" width="300"/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="/assets/img/python-shm.gif" alt="isolated" width="350"/></p>

In addition to [Cadabra](https://cadabra.science/), I used [Mathematica](https://www.wolfram.com/mathematica/) and [Maple](https://www.maplesoft.com/) for my research in quantum gravity during my PhD and beyond. I have also used various programming languages or libraries (e.g NumPy, SciPy, Matplotlib etc) to perform mathematical, physical, and statistical computations for various analyses and datasets. 

<p align="center"><img src="/assets/img/pylib.png" alt="isolated" width="350"/></p>

During my PhD, I also worked on a personal project to interact with bluetooth low energy (BLE) smart watches in Python. This was used to  read the heart rate (bpm), systolic/diastolic blood pressure (mmHg) and SpO2 blood oxygen (%) with a live plot of real-time HR readings using gnuplot. I also used Python to perform data analysis on the data collected from the smart watch.

<p align="center"><img src="/assets/img/ble.gif" alt="isolated" width="450"/></p>

After leaving academia, one of the roles I decided to pursue was a Scientific Software Engineer at the [Meteorological Office](https://www.metoffice.gov.uk/). For this role, I gave a presentation on the history, applications and good practices of scientific software engineering.

<p align="center"><a href="https://www.metoffice.gov.uk/"><img src="/assets/img/mo.png" alt="isolated" width="100"/></a></p>

Ultimately I decided to embark on a career in data science instead. Nevertheless, the panel was very impressed with my application and noted that I had a good understanding of the importance of software quality, and awareness of the challenges of developing scientific software. I also demonstrated good examples of software development, especially in Python and a sensible approach to debugging code, including when working with other’s code. 

I am experienced with data science and machine learning Python libraries (e.g Scikit-learn, Keras, Tensorflow) as well as data visualization software such as Tableau and Power BI. 

<p align="center">
<img src="/assets/img/scikit.png" alt="isolated" width="125"/>
<img src="/assets/img/tflow.png" alt="isolated" width="300"/>
</p>

As a freelance data scientist, I analysed instantaneous power consumption data of a large number of households with supervised machine learning models to identify various devices (e.g the TV or kettle) and classify when they are turned on and the occurrences/duration of their usage, to identify routines and the detection of anomalies. This data was provided by a particular company that aims to use ML and modelling with domestic electric appliances.

<p align="center"><img src="https://static.wixstatic.com/media/e465c3_63f788f48f2a42c1ab6736fe40180f9e~mv2.png/v1/fit/w_829%2Ch_456%2Cal_c/file.png" alt="isolated" width="250"/></p>

<a name="ons_journey"></a>
# My journey at the Office for National Statistics

Throughout my tenure at the [Office for National Statistics](https://www.ons.gov.uk/) (ONS), I have made pivotal contributions to data science and analysis, specializing in the development of advanced statistical and machine learning models, designing analytical data pipelines, and crafting data visualizations to distil complex data insights. Key projects include devising sophisticated multilateral price indices for the treatment of alternative data sources, implementing novel machine learning methodologies in Python, and significantly enhancing the computational efficiency of data processes. Furthermore, my commitment to knowledge sharing has positioned me as a mentor to new team members and a key presenter in various organizational seminars.

<p align="center"><a href="https://www.ons.gov.uk/"><img src="/assets/img/ons.png" alt="isolated" width="275"/></a></p>

### Data Scientist in `Reproducible Data Science and Analysis`

In **June 2021**, I was employed as a Data Scientist at the *Higher Executive Officer* grade at the ONS within the Economics Statistics Group (ESG) and the Reproducible Data Science and Analysis (RDSA) team, formerly known as Emerging Platforms Delivery Support (EPDS). 


After only my second month at the ONS, I was a member of the induction team responsible for onboarding new starters and aiding or mentoring to new members of the team. My main work was researching and implementing [multilateral](https://cran.r-project.org/web/packages/IndexNumR/vignettes/indexnumr.html#multilateral-index-numbers) price indices, using calculations and time series extension methods in Python. This work was as part of an ETL Reproducible Analytical Pipeline (RAP) on Cloudera with Apache Spark for the treatment of [alternative data sources](https://www.ons.gov.uk/economy/inflationandpriceindices/articles/introducingalternativedataintoconsumerpricestatisticsaggregationandweights/2021-11-09) (scanner and web-scraped data) and [new index methods](https://www.ons.gov.uk/economy/inflationandpriceindices/articles/newindexnumbermethodsinconsumerpricestatistics/2020-09-01) which will be used to determine the consumer price index (CPI) in the future. 

<p align="center"><img src="/assets/img/spark.png" alt="isolated" width="400"/></p>

As part of the [new index methods](https://www.ons.gov.uk/economy/inflationandpriceindices/articles/newindexnumbermethodsinconsumerpricestatistics/2020-09-01), I had been looking at mutlilateral methods which simultaneously make use of all data over a given time period. Their use for calculating temporal price indices is relatively new internationally, but these methods have been shown to have some desirable properties relative to their [bilateral](https://cran.r-project.org/web/packages/IndexNumR/vignettes/indexnumr.html#bilateral-index-numbers) method counterparts, in that they account for new and disappearing products (to remain representative of the market) while also reducing the scale of chain-drift.

While working on building a data pipeline for the CPI, I made very significant contributions both to methodology and computational efficiency for the integration of alternative data sources. In my first few months, I led an investigation into a particular implicit hedonic multilateral index method known as the [Time Product Dummy](https://onlinelibrary.wiley.com/doi/full/10.1111/roiw.12468#roiw12468-sec-0002-title) (TPD) method, which uses a log-linear price model with weighted least squares regression and expenditure shares as weights.

$$
\begin{aligned}
\ln p_i^{t} &= \alpha + \sum_{r=1}^T \delta^r D_i^{t,r} + \sum_{j=1}^{N-1}\gamma_j K_{i,j} + \epsilon_i^{t} \ , \\
s_i^{t} &= \frac{p_i^t q_i^t}{\sum_{j=1}^N p_j^t q_j^t}  \ .
\end{aligned}
$$

After noticing an error in the formulae and example workbooks produced for these methods and bringing this to the attention of the ONS, I worked closely with people from methodology on making sure we got all the technical details right.

My first task was to implement the TPD method within the CPI pipeline using PySpark. Spark's native ML library though powerful generally lacks many features, and is not suited for modelling on multiple groups or subsets of the data at once. The usual approach to use custom functions or transformations which are not part of the built-in functions provided by Spark’s standard library is to use a User Defined Function (UDF). However, the downside of this is they have performance issues, since they executed row-at-a-time and thus suffer from high serialization and invocation overhead.

<p align="center"><img src="/assets/img/pyudf.png" alt="isolated" width="300"/></p>

This led me toward discovering [Pandas UDFs](https://www.databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html), which allow for vectorized operations on Big Data and increase performance by up to 100x compared to regular UDFs using Apache Arrow. They have since been implemented in various multilateral index methods and are an integral part of the CPI pipeline. 

<p align="center"><img src="/assets/img/warrow.png" alt="isolated" width="500"/></p>

<p align="center"><a href="https://www.databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html"><img src="/assets/img/pudf_performance.png" alt="isolated" width="500"/></a></p>

I also used the same ideas for the [Time Dummy Hedonic](https://onlinelibrary.wiley.com/doi/full/10.1111/roiw.12468#roiw12468-sec-0002-title) (TDH) method, which is an explicit hedonic model similar to TPD, but also uses the item characteristics in the WLS regression model. 

$$
\begin{aligned}
\ln p_i^{t} = \delta^0 + \sum_{r=1}^T \delta^r D_i^{t,r} + \sum_{k=1}^K \beta_k z_{i,k} + \epsilon_i^{t} \ . 
\end{aligned}
$$

After implementing the TPD and TDH methods, I turned my attention to another multilateral method known as [Geary-Khamis](https://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/a-comparison-of-ten-methods-for-multilateral-international-price-and-volume-comparison.pdf) (GK) and the usual method involves iteratively calculating the set of quality adjustment factors simultaneously with the price levels. 

$$
\begin{aligned}
b_{n}&=\sum_{t=1}^{T}\left[\frac{q_{t n}}{q_{n}}\right]\left[\frac{p_{t n}}{P_{t}}\right] \ ,
\nonumber \\
P_{t}&=\frac{p^{t} \cdot q^{t}}{ \vec{b} \cdot q^{t}} \ .
\end{aligned}
$$


I was able to independently research and implement a method solely based on [matrix operations](https://drrobotk.github.io/2021-09-20-Geary-Khamis/), which makes the method more efficient since it has vectorized operations which act on the entire data. I also refactored my code for TPD and TDH using matrix operations, which turned out to be more efficient and increased performance by up to 7x compared to standard statistical libraries. The Pandas UDFs were also applied to the time series [extension methods](https://unece.org/sites/default/files/2021-05/Session_1_Netherlands_Paper.pdf) for TPD, TDH, GK and another multilateral method known as [GEKS](https://unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.22/2016/Session_1_room_doc_Netherlands_an_overview_of_price_index_methods.pdf). 

In **October 2021**, after working closely with methodology on index numbers, I was invited to join the [Index Numbers Expert Group](https://analysisfunction.civilservice.gov.uk/government-statistical-service-and-statistician-group/gss-support/methodology/information-on-specific-methods/#index-numbers) (INEG) and the [Data Science and High-performance computing](https://analysisfunction.civilservice.gov.uk/government-statistical-service-and-statistician-group/gss-support/methodology/information-on-specific-methods/#data-science-and-high-performance-computing) (DaSH) expert group. 

In **November 2021**, I delivered a presentation in a seminar to my team and deparment, to introduce the concept of Pandas UDFs. This turned out to be a success as I got good engagement and questions after the presentation, as well as interest from other parties in DaSH, to watch the recording and slides. I also presented a seminar aimed at people both little and extensive knowledge of the subjects, and a Jupyter Notebook of worked examples. I discussed this material with a computing specialist, and with their feedback have produced useful material with a full set of instructions and worked examples, which is accessible to a wider audience.

### Senior Data scientist in the `Data Science Campus`

In **March 2022**, I [joined](https://datasciencecampus.ons.gov.uk/author/usman-kayani/) the [Data Science Campus](https://datasciencecampus.ons.gov.uk/) at the ONS with a promotion to *Senior Executive Officer* and a permanent role in the civil service.

<p align="center"><a href="https://datasciencecampus.ons.gov.uk/"><img src="/assets/img/dsc.png" alt="isolated" width="200"/></a></p>

My first project was on the [least cost index](https://www.ons.gov.uk/economy/inflationandpriceindices/articles/trackingthelowestcostgroceryitemsukexperimentalanalysis/april2021toapril2022), which was published in May 2022. I played a significant role in researching and implementing the price index and aggregation methods, which was powered by a Python price index package which I created called [PriceIndexCalc](https://pypi.org/project/PriceIndexCalc/).

<p align="center"><a href="https://www.ons.gov.uk/economy/inflationandpriceindices/articles/trackingthelowestcostgroceryitemsukexperimentalanalysis/april2021toapril2022"><img src="/assets/img/lci.png" alt="isolated" width="650"/></a></p>

My package and work was used to track the prices over time of the lowest-cost grocery items for 30 products over multiple retailers, using web-scraped data and a data pipeline on the [Google Cloud Platform](https://cloud.google.com/). This analysis was conducted as part of the ONS's current and future analytical work related to the cost of living.

<p align="center"><a href="https://cloud.google.com/"><img src="/assets/img/gcp.jpg" alt="isolated" width="150"/></a></p>
<p align="center"><img src="/assets/img/bucket.png" alt="isolated" width="600"/></p>

In **April 2022**, I also joined the Data Access Platform Capability And Training Support (DAPCATS) as a mentor, where I have been helping other data scientists and analysts with their work and projects. 

<p align="center"><img src="https://best-practice-and-impact.github.io/ons-spark/_static/logo.png" alt="isolated" width="200"/></p>


I also took part in the *Spark at the ONS* event hosted by DAPCATS and created for the launch of a new [online book](https://best-practice-and-impact.github.io/ons-spark/intro.html). This event was used to discuss various topics and resources related to Spark and Big Data, and I delivered a presentation titled *Spark application debugging, tuning and optimization*. For this talk, I discussed various tips and techniques to increase efficiency, identify bugs or bottlenecks that can cause Spark applications to be slow or fail, and tuning Spark parameters accordingly. This can help to reduce overall developer and compute time, costs for resources to run the Spark application or the environmental impact that comes with using unnecessary extra resources or having significantly longer runtimes. 

In **August 2022**, I received the Recognition Award for outstanding collaboration and contribution to the ONS. I provided very important support to help another team to publish the [Capital Stocks user guide](https://www.ons.gov.uk/releases/introducingthecapitalstocksuserguide) article and the work has made the UK the only country to introduce such transparency. The process involved sharing their [statistical production code](https://github.com/ONSdigital/Capstocks) in the ONS's GitHub account and I dedicated my time to help them set up the initial account, and to upload the packages in GitHub as the team hadn't experienced using this platform before. I also took the time to give them a very detailed walk through of how the platform works, and helped them by sharing tips and examples of good practice. My support enabled them to make their capital stocks statistical production system accessible and reproducible by all external users, helping them make the statistics more inclusive and introducing innovating platforms to help their users improve their analysis and budgetary forecasting.

In **September 2022**, I continued to work on a project to investigate the feasibility of using transparency declarations to improve intelligence on public sector expenditure and increase the quality of ONS public statistics. The declarations refer to expenditure data that local councils and central government bodies must publish to meet their transparency requirements. This work may also offer insights into the spatial distribution of public spending, which could be useful for policy agendas.

In **October 2022**, I became a founding member of the ONS Data Science Network, a new cross-departmental group that promotes data science events and training across the organization. The network also provides a forum for data scientists and analysts to discuss and share ideas on data science and analysis, and to promote the use of data science and analysis. The network consists of founding members from the Reproducible Data Science and Analysis (RDSA), Methodology and Quality Directorate (MQD), and Data Science Campus (DSC).

In **November 2022**, I had a one-on-one chat with [Professor Sir Ian Diamond](https://www.gov.uk/government/people/ian-diamond), the National Statistician and head of the ONS, about ways to improve transparency for statistics. We discussed the importance of releasing code for scrutiny and learning purposes, as well as the challenges that prevent people from releasing code. We also talked about the potential for the ONS Data Science Network to promote code quality for data science projects across the organization. Additionally, we discussed the importance of data visualization and communication in transparency and the ONS's efforts to improve in this area.

In **December 2022**, I received the Recognition Award again for outstanding collaboration and contribution to the ONS. I helped another team in a different department who recently migrated their systems and team of new developers to GCP. They encountered issues getting things set up with the on-prem laptop and GCP, largely due to niche ONS system restrictions which made it difficult to find resources on the internet to solve them. I generously shared my knowledge and expertise, which saved the team a lot of time and helped them gain a deeper understanding of the topic.

In **May 2023**, despite leaving the ONS in January of the same year, I was honored with another Recognition Award for the impactful contribution I had made in establishing a cross-ONS network of data scientists during my time at the organization. Displaying initiative, I set up a project management tool to handle different aspects of the network and its prospective deliverables. Moreover, I created a dedicated communication channel and a network inbox, both of which were critical for effective communication within the network. My proactive role in laying the foundation for the network was acknowledged as instrumental in creating a thriving environment where data scientists across the ONS could collaborate and learn. This achievement not only acknowledges my contributions to the data science community within the ONS but also underscores the importance of fostering collaboration and community in the ever-evolving field of data science.

<a name="quantexa_journey"></a>
# My journey at Quantexa

My role as a Data Engineer at [Quantexa](https://www.quantexa.com/) has been a deep dive into the exciting and challenging world of data engineering. This journey has allowed me to push the boundaries of big data technology, harnessing its transformative potential to enhance decision-making for businesses across multiple sectors. My time at Quantexa has been marked by perseverance, ambition, teamwork, and accountability—principles that resonate deeply with the company’s core values. From optimizing processes with Python scripts to identifying system vulnerabilities, I have consistently sought impactful contributions. My approach to teamwork emphasizes inclusivity, fostering an environment that champions collective growth. Accountability remains a priority, maintained through regular feedback and high standards of continual self-improvement.


<p align="center"><a href="https://www.quantexa.com/"><img src="https://www.quantexa.com/images/quantexa.svg" alt="isolated" width="350"/></a></p>

## Data Engineer in Research and Development (R&D)

In January 2023, I joined Quantexa as a Data Engineer, diving into advanced network analytics and dynamic entity resolution in a dynamic fintech setting. With its 2016 founding, Quantexa has established itself as a leader in contextual data insights, applying its pioneering technologies across diverse sectors like Finance, Insurance, Energy, and Government to tackle issues from lead generation and customer insights to fraud detection and financial crime. It’s inspiring to contribute to an organization driven by the belief that better decisions are made through contextual understanding.

My role has been dynamic and impactful, encompassing the development, testing, and documentation of data engineering tools and best practices. These materials support Quantexa’s deployments, strengthening the data engineering function and improving the quality and efficiency of project delivery.

Though relatively new to Scala, I’ve made significant strides in mastering this language, a vital tool for data processing within big data ecosystems. My proficiency in Scala has produced tangible results, with contributions that influence development sprints and code optimization. My knowledge of big data technologies—particularly Spark, Hadoop, and Elasticsearch—has also been pivotal in defining best practices across the business. With skills in Java, Python, and Scala, I’m well-equipped to support the team’s initiatives and deliver efficient solutions across both cloud and on-premise environments.

Stakeholder engagement is essential to my role, allowing me to work closely with delivery teams, clients, and partners to deliver high-quality solutions. These solutions span diverse tasks, including ETL pipelines, data cleansing, parsing, and standardization, as well as data classification and entity extraction/resolution.

A significant milestone in my journey was achieving a 97% score in the Quantexa Academy, earning the title of Quantexa Certified Data Engineer. This accomplishment deepened my expertise in Quantexa’s technology stack, and I actively support other Academy participants, fostering a collaborative learning environment within the team.

# Senior Data Engineer in Research and Development (R&D)

In October 2024, I was promoted to Senior Data Engineer within Quantexa’s Data Engineering Accelerators and Demos (DEAD) team in R&D. This role enables me to lead complex data engineering projects and drive innovation within Quantexa’s technology stack. My responsibilities now include architecting, developing, and optimizing scalable data pipelines that support large-scale data integration, advanced analytics, and entity resolution. I work across both cloud and on-premise infrastructures, leveraging tools like Airflow, Spark, and Google Cloud, with a strong focus on infrastructure optimization, automation, and performance.

A recent highlight was spearheading the implementation of dynamic Directed Acyclic Graphs (DAGs) and autoscaling in Airflow, which has optimized resource usage and boosted scalability. This project aligns with my technical focus on big data and cloud solutions, which allows me to refine Quantexa’s infrastructure management practices for improved accessibility and best practices across the team.

Mentorship has become a rewarding focus of my role. I work closely with junior engineers, guiding them through coding challenges, conducting code reviews, and creating a collaborative learning environment. My expertise in Scala, Python, and Java has been instrumental in tackling complex data engineering problems and supporting the professional growth of emerging engineers, especially as we manage large-scale data and complex systems.

Innovation is central to my role, and I’ve recently developed a dynamic DAG generator, allowing ETL pipelines to be created directly from configuration files, which has streamlined Quantexa’s ETL workflow and enabled rapid deployment of custom data pipelines. My vision is to expand this into a web-based application with a drag-and-drop interface, empowering data engineering teams to design, configure, and deploy ETL pipelines more efficiently. This project represents my commitment to creating accessible tools that democratize data engineering workflows and enhance productivity.

Beyond technical work, I’m responsible for setting and implementing best practices in ETL pipelines, big data processing, and data standardization. These initiatives ensure Quantexa’s ability to provide high-quality, standardized data for accurate analytics and decision-making. My experience with Spark, Hadoop, and Elasticsearch supports strategic efforts like data classification and entity extraction, both of which are integral to Quantexa’s data solutions.

I am also a strong advocate for neurodiversity and inclusivity within Quantexa, particularly for ADHD and dyslexic perspectives. I bring this advocacy to my work by emphasizing clear documentation, accessible resources, and continuous learning opportunities, ensuring that all team members can contribute effectively.

As I continue my journey at Quantexa, I look forward to the new challenges, learning opportunities, and contributions to data engineering. I am dedicated to aligning my work with Quantexa’s mission of delivering actionable insights through contextualized data, and I am excited to drive even greater successes with the company in the years to come.









